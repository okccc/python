- [cdh各组件端口](https://docs.cloudera.com/documentation/enterprise/6/6.2/topics/cdh_ports.html)
### hdfs
```bash
# client：客户端
1.将文件按block块切分
2.与namenode交互,获取文件的位置信息
3.与datanode交互,读写数据
# namenode：管理节点
1.管理hdfs命名空间
2.管理元数据信息,即文件与数据块的映射关系
3.配置数据块副本 
4.处理客户端读写请求
# datanode：工作节点,执行namenode下达的命令
1.存储实际的数据块
2.执行数据块的读写操作
# secondarynamenode：为了避免edits不断变大,会定期合并fsimage和edits,该操作挺耗时,在namenode操作会影响性能导致卡顿
fsimage是某一时刻hdfs的快照
edits会记录hdfs的各种更新操作
# balancer
使hdfs各节点的空间使用率均匀分布
        
# 写数据
# block大小 dfs.block.size=128m
# packet大小 dfs.write.packet.size=128k
1.客户端请求namenode上传文件,namenode返回是否可以上传
2.客户端请求上传第一个block的datanode服务器,namenode返回dn1/dn2/dn3数据节点
3.客户端请求向dn1上传数据,dn1收到请求会继续调用dn2,然后dn2调用dn3,建立通信管道pipline
4.客户端开始往dn1上传第一个block,以packet为单位,dn1收到一个packet就会传给dn2,dn2传给dn3 
5.当一个block传输完成之后,客户端再次请求namenode上传第二个block的服务器

# 读数据
1.客户端请求namenode下载文件
2.namenode查询元数据找到文件块存放地址,就近挑选一台dn节点读取数据
3.datanode开始传输数据给客户端

# 为什么hdfs是3个副本？
hdfs采用机架感知策略来保证数据的可靠性并减少网络传输,一个副本存放在本地机架节点,另一个副本存放在本地机架的另一个节点,第三个副本存放在不同机架的节点
```

### hive
```bash
# hive、hiveserver2、beeline
hive和beeline都是hive客户端,hiveserver和hivesrver2都是hive服务端
hiveserver是本地模式,只能处理单个请求,已废弃
hiveserver2是远程连接模式,可以在任意机器使用hive-cli/beeline/hue/jdbc连接
# gateway
由于hive服务没有worker角色,需要另一种机制使客户端的配置传播到集群中其它主机
# metastore
存储hive元数据,一般存放在关系型数据库mysql中,内置derby只支持一个会话连接
# driver
解析器(SQL Parser)：将sql字符串解析成抽象语法树AST并进行语法分析
编译器(Compiler)：将AST编译成logical plan
优化器(Optimizer)：优化logical plan
执行器(Executor)：将logical plan转换成可执行的physical plan,调用MR/Spark计算框架

# 数据库以行列二维表形式展现数据,以一维字符串方式存储数据
# record-oriented
把一行的数据值串在一起存储,然后再存储下一行(1,zhangsan,18 2,lisi,19 3,wangwu,20 ...)
读数据会读取一整行有些冗余,写数据会一次性写入速度很快,适合写多读少面向oltp(mysql)
# column-oriented
把一列的数据值串在一起存储,然后再存储下一列(1,2,3 zhangsan,lisi,wangwu 18,19,20 ...)
读数据只会读取相关列避免全表扫描且无需维护索引,写数据要把一行数据拆成单列多次写入消耗更大,适合读多写少面向olap(hive/hbase)
同一列的重复数据和空值可以提高压缩率,查询时直接作用于某一列聚合分析也很快
# 文件存储格式
textfile和sequencefile基于行存储(不常用)
orc和parquet基于列存储,parquet支持hadoop生态圈的所有项目,orc支持集合、映射等复杂数据结构(常用)
```
```sql
-- beeline也是hive客户端连接工具,显示格式更友好,后续会替代hive-cli
beeline> !connect jdbc:hive2://master1:10000
connecting to jdbc:hive2://master1:10000
enter username for jdbc:hive2://master1:10000: 
enter password for jdbc:hive2://master1:10000: 
connected to: apache hive (version 1.1.0-cdh5.14.2)
driver: hive jdbc (version 1.1.0-cdh5.14.2)
transaction isolation: transaction_repeatable_read
0: jdbc:hive2://master1:10000> show databases;
+----------------+--+
| database_name  |
+----------------+--+
| default        |
| dm             |
| dw             |
| ods            |
| tmp            |
| views          |
+----------------+--+
6 rows selected (0.293 seconds)
0: jdbc:hive2://master1:10000> !quit
closing: 0: jdbc:hive2://master1:10000
```

### impala
```bash
# d(daemon)表示守护进程,是运行在linux后台的一种服务程序,周期性地执行某种任务或等待处理某些事件,linux的大多数服务就是用守护进程实现的
# impala是基于hive的实时分析查询引擎,直接使用hive的元数据库metadata
# hive和impala对比
hive适合长时间的批处理,有大量读写磁盘的过程 map->shuffle->reduce->map->shuffle->reduce
impala适合实时查询,中间结果放内存通过网络传输没有磁盘读写所以速度更快

# impala daemon(核心组件,多实例,物理进程impalad)
负责读写数据,接收从impala-shell/hue/jdbc等接口发送的查询sql,并行查询并在集群中分配任务,将中间结果集发送给协调器
impala daemon通常和datanode部署在相同节点,接收查询的节点作为协调器coordinator,其它节点会传输中间结果集给协调器并由协调器构建最终结果集
impala daemon会不间断地和statestore通信以确认哪些节点是健康的能接收新的任务,同时接收catalog传来的广播消息更新元数据

# impala statestore(单实例,物理进程statestored)
检查集群各节点impala daemon的健康状态

# impala catalog(单实例,物理进程catalogd)
当metadata更新时会通知任意impala daemon刷新元数据信息,因为会与statestore交互所以安装在同一节点
```

### shell
```bash
# hive shell
-d,--define<key=value>         # 定义变量 -d num=10 
-e,<quoted-query-string>       # 执行一段sql
-f,--filename                  # 执行保存sql的文件           
-h,--hostname                  # 连接远程hive server           
-p,--port                      # 连接远程hive server端口号      
-hiveconf,<property=value>     # 设置配置参数                   
-hivevar,<key=value>           # 类似define                    
-s,--silent                    # 安静模式,只显示结果不显示进度

# impala-shell
-b,--delimited                 # 去格式化输出,大数据量查询可以提高效率
-i,--impalad                   # 连接指定impalad,impala-shell端口21000,hue/jdbc端口21050
-d,--database                  # 指定数据库
-q,--query                     # 执行一段sql
-f,--filename                  # 执行保存sql的文件
-o,--output                    # 输出结果到指定文件
-v/-v,--version/--verbose      # 查看版本信息,开启详细输出
--quiet                        # 安静模式,只显示结果不显示进度
```

### cmd
```sql
-- 删除库(加cascade可以删除含有表的数据库)
hive> drop database test cascade;
-- 模糊搜索表
hive> show tables like '*name*';  
-- 删除表
hive> drop table test;
-- 清空表数据
hive> truncate table test;
-- 添加字段(注意：添加新字段后要将原来已经存在的分区先删掉,不然数据加载不进去,如果要调整新字段顺序,可以再用change)
hive> alter table test add columns(order_id int comment '订单id'); 
-- 修改字段
hive> alter table test change column column1 column2 string comment '...' first|after column3;  
-- 删除字段(只保留需要的列,不需要的列删掉,同时也可以更换列的顺序)
hive> alter table test replace columns(id int, name string);  
-- 删除表分区,如果直接删除hdfs数据目录,表分区还在但没有数据 
hive> alter table test drop partition (dt=20160101);               # 删除单个分区
hive> alter table test drop partition (dt>=20160101,dt<20170101);  # 删除多个分区
-- 重命名表
hive> alter table table1 rename to table2; 
-- temporary表示临时表,仅在本次hive session期间有效,关闭hive后会自动删除,不加该关键字则会存储下来
-- create table like 复制表结构(没有数据)
hive> create temporary table table1_tmp like table1 stored as textfile;
-- create table as 生成新表并插入数据,表结构取决于select的内容
hive> create temporary table table1_tmp as select * from table1;
-- 查看分区信息
hive> show partitions test;  
-- 查看最小分区
hive> select min(dt) from test;
-- 查看系统当前用户
hive> set system:user.name;
system:user.name=root
-- 查看某个参数默认配置
hive> set mapred.reduce.tasks;
-- 在输出结果最上面一行打印列名  
hive> set hive.cli.print.header=true;
-- 查找所有函数 
hive> show functions;
-- 查看某个函数使用案例  
hive> desc function extended parse_url;
-- 视图
hive> create view v01 as select * from debit_info where dt=regexp_replace(date_sub(current_date,1),'-','');  
-- 注册udf,将开发的udf打成jar包上传到hdfs指定目录,然后创建函数  
hive> create function default.url_decode as 'com.qbao.udf.decodeurl' using jar '<hdfs:///lib/decodeurl.jar>';

-- hive查看表统计信息
hive> desc formatted table_name;
-- hive计算表和字段的统计信息,分区表必须指定分区(不可靠且难用,建议用impala的compute stats)
hive> analyze table table_name [partition(dt=20200612)] compute statistics;  -- 表
hive> analyze table table_name [partition(dt=20200612)] compute statistics for columns;  -- 列

-- impala连接指定主机
[master2:21000] > connect cdh2:21000;
-- 刷新指定表
[master2:21000] > refresh table_name;
-- 刷新元数据所有表
[master2:21000] > invalidate metadata;
-- 查看执行计划
[master2:21000] > explain select;
-- 设置查询计划显示级别
[master2:21000] > set explain_level=0/1/2/3;  -- 等级越高越详细
-- 执行查询sql后再执行summary或profile可以查看详细查询分析
[master2:21000] > summary/profile;
-- impala优化器：hive数据更新 - refresh - compute stats 会统计一些聚合信息并存储在元数据中
-- impala关联查询优化：compute stats收集统计信息后,impala会基于每个表的大小、每一列的不同值个数等信息优化查询计划
-- 建议最大表放首位,因为这个表是直接从磁盘读取,它的大小不影响内存使用,后续join的表作为中间结果都是放在内存
[master2:21000] > compute/drop stats table_name;  -- 全量
[master2:21000] > compute/drop incremental stats table_name partition(dt='20200612' | dt>'20200101' | dt<'20200612');  -- 增量
-- impala查看表和字段统计信息
[master2:21000] > show column stats table_name;   -- 查看字段聚合信息
[master2:21000] > show table stats table_name;  -- 查看表聚合信息,包括文件行数/大小/类型/路径等

-- mysql数据导入
mysql> show variables like 'secure_file_priv';
mysql> load data local infile '...' [replace] into table test;  # 覆盖/追加 
-- mysql数据导出
mysql> select * from test into outfile '...' fields terminated by ',' enclosed by '"' lines terminated by '\n';  
-- hive数据导入
hive> load data [local] inpath '...' [overwrite] into table t1 [partition(dt='..')]  # 本地(复制)/hdfs(剪切)  覆盖/追加    
hive> insert overwrite/into table t1 [partition(dt=20200101)] select * from t2 where ...
hive> create table t2 as select * from t1 where ...
-- hive数据导出
hive -e "select * from test;" > /opt/aaa.txt  # insert overwrite慎用,会覆盖整个目录!

-- 排序方式
hive> select * from test;  
5 3 6 2 9 8 1  
-- order by：全局排序,最后会用一个reduce task来完成
hive> select * from test order by id;  
1 2 3 5 6 8 9  
-- sort by：分区内排序,在每个reduce内部排序,如果reduce.task=1,等价于order by
-- distribute by：对map输出数据按指定字段划分到不同reduce中,常和sort by一起使用
hive> set mapred.reduce.tasks=2;  
hive> select * from test distribute by id sort by id;  
2 5 6 9 1 3 8    
-- cluster by：当distribute by 和sorts by字段相同时可使用cluster by代替,但是只能升序排序
```

### tables
```sql
-- hive是把除了类似select * 这种简单查询以外的sql都翻译成mapreduce在yarn集群里跑

-- 内部表,默认路径hive.metastore.warehouse.dir=/user/hive/warehouse,并且还会生成一个tmp目录
-- 内部表数据由hive管理,删除表会直接删除hive元数据metadata及存储数据
create table if not exists dw.inner(  
json string  
)  
comment ''  
row format delimited  
fields terminated by '\t'  
lines terminated by '\n'  
-- orc：先将数据按行分块再按列存储,保证同一条记录在一个块上避免读取多个数据块,snappy可以极大节省存储空间
stored as orc tblproperties ("orc.compress"="snappy");
-- 注意：sqoop从mysql导到hive的表不能是orc格式,不然无法访问数据,可以先导到textfile临时表再insert overwrite/into到orc表

-- 外部表,可以指定路径
-- 外部表数据由hdfs管理,删除表只会删除hive元数据metadata,hdfs上的文件还在
create external table if not exists dw.external(
json string  
)  
comment ''  
row format delimited  
fields terminated by '\t'  
lines terminated by '\n'  
stored as orc tblproperties ("orc.compress"="snappy")  
location 'hdfs://nameservice1/user/flume/qbao_goods_stuff_log';

-- 分区表可以在海量数据当中快速检索到需要的数据
-- 内部表和外部表都可以设置分区,指定location就是外部分区表  
-- 分区：以字段的形式在表结构中存在,但该字段不存放实际数据内容,仅仅是分区标识
-- 分桶：更为细粒度的数据范围划分,将整个数据按照某个列的hash值划分到指定数量的桶里(不常用)
create table if not exists ods.rec_spu_summary(  
uid                 int,  
uname               string,  
spu_id_list         array<int>,  
request             map<int, string>
)  
partitioned by (dt string)  
row format delimited  
fields terminated by '\001'  -- 不同列以'\001'分隔,集合(array,map)元素之间以'\002'分隔,map中key和value以'\003'分隔;  
lines terminated by '\n'  
stored as textfile;  

-- 动态分区
-- 业务需求：mysql表很大,现在要抽到hive按天分区,保留2016年后的数据,2016年以前的数据都放到20151231这个分区里
-- 解决方法：先将全量数据导入到temp的临时表(不分区),然后使用动态分区插入到ods层的分区表中
-- 注意：动态分区的字段一定位于其他各个字段的最后
set hive.exec.dynamic.partition=true;                  -- 开启动态分区
set hive.exec.dynamic.partition.mode=nonstrict;        -- 默认strict(严格模式,必须至少包含一个静态分区) 
set hive.exec.max.dynamic.partitions=10000;            -- 能生成的动态分区最大总数  
set hive.exec.max.dynamic.partitions.pernode=1000;     -- 每个节点能生成的最大分区数 
insert overwrite table ods.tickets_order partition(dt)
select *,  
       case when create_time >= '2016-01-01' then regexp_replace(substr(create_time,0,10),'-','') else 20151231 end
from temp.tickets_order;
```

### join
```sql
       id     id
1   a   3      1   xxx   2
2   b   4      2   yyy   3 
3   c   1      3   zzz   5

-- 内连接(id交集)
hive> select * from a join b on(a.id = b.id);  
3   c   1   1   xxx   2  
1   a   3   3   zzz   5  

-- 左连接(左表是主表)
hive> select * from a left join b on(a.id = b.id);  
1   a   3   3   zzz   5  
2   b   4   null null null  
3   c   1   1   xxx    2  

-- 左半开连接：只显示匹配到的左表数据,比左连接快
hive> select * from a left semi join b on(a.id = b.id);  
1   a   3  
3   c   1  

-- 右连接(右表是主表)
hive> select * from a right join b on(a.id = b.id);  
3   c   1       1   xxx   2  
null null null  2   yyy   3  
1   a   3       3   zzz   5  

-- 全连接(id并集)
hive> select * from a full join b on(a.id = b.id);  
3   c   1      1   xxx   2  
null null null 2   yyy   3  
1   a   3      3   zzz   5  
2   b   4      null null null  

-- 笛卡尔积：m*n(没有关联条件)
hive> select * from a join b;  
1   a   3   1   xxx   2  
2   b   4   1   xxx   2  
3   c   1   1   xxx   2  
1   a   3   2   yyy   3  
2   b   4   2   yyy   3  
3   c   1   2   yyy   3  
1   a   3   3   zzz   5  
2   b   4   3   zzz   5  
3   c   1   3   zzz   5  
```

### skew
```sql
-- 数据倾斜本质：shuffle过程中map端的输出结果按照key hash分配不均匀,导致reduce端分到的数据量差异过大
-- 1.join导致
set hive.optimize.skewjoin=true; 
set hive.skewjoin.key=100000;  -- join的键对应的记录数超过设定值会做map join 
-- a)关联字段存在null值
select * from users a left join orders b on a.uid is not null and a.uid = b.uid
union all  
select * from users a where a.uid is null;
-- 上面方案需要2个job,下面方案只要1个job  
select * from users a left join orders b on case when a.uid is null then concat('hive',rand()) else a.uid = b.uid;  
-- b)关联字段类型不一致,类型转换时会产生null值
select * from users a left join orders b on a.uid = cast(b.uid as int);  
-- c)小表前置
hive join默认是reduce join,如果左表很小可以将其加载到内存做map join,省掉shuffle过程

-- 2.group by导致
set hive.map.aggr=true;  -- 在map端先做部分聚合操作
set hive.groupby.mapaggr.checkinterval=100000;   -- groupby的键对应的记录数超过设定值就会在map端聚合 
set hive.groupby.skewindata=true;  -- 生成的查询计划会有两个mrjob,第一个job中map的输出结果会随机分配到reduce,每个reduce做聚合操作并输出结果,这样即使相同的groupby key也会被分配到不同的reduce,从而达到负载均衡,第二个job再根据预处理的数据结果按照groupby key分配到reduce,这个过程可以保证相同的groupby key被分配到同一个reduce,完成最终聚合操作  
-- multi-group可以将多个group by放到一个mapreduce
from area
insert overwrite table t1 
select provice,count(rainfall) from area where dt=20200101 group by provice
insert overwrite table t2
select provice,city,count(rainfall) from area where dt=20200101 group by provice,city

-- 3.count(distinct ..)导致
count distinct属于全局操作,最终会用一个reduce处理导致耗时过长,可以先用group by去重
select count(id) from (select id from users group by id) a;  -- 虽然会多一个job但是数据量很大时还是值得的
```

### optimize(粗粒度)
```sql
-- hive优化本质上是mapreduce优化
-- map join操作小表放左边,会被加载到内存,在map端做join操作而不是reduce端,可以省去shuffle过程大量io操作  
-- map和reduce数量不是越多越好,启动和初始化很消耗时间和资源;并且有多少个reduce就会有多少个output文件,迭代过程中大量output文件又会成为下个任务的input
-- job会通过input文件产生map任务,map数和文件大小,文件个数,文件块大小(默认128m,set dfs.block.size)有关  
-- 原则：使大数据量使用合适的map/reduce数,使单个map/reduce任务处理合适的数据量

-- 1.减少map数  
-- inputdir /user/hive/warehouse/test/dt=20170101 共194个文件总大小9g,其中很多<<128m的,正常执行会占用194个map任务,消耗计算资源：slots_millis_maps=623020  
select count(1) from test where dt=20170101;  
-- 设置块大小
set mapred.max.split.size=100000000;  
set mapred.min.split.size.per.node=100000000;  
set mapred.min.split.size.per.rack=100000000;  
-- 合并小文件  
set hive.input.format=org.apache.hadoop.hive.ql.io.combinehiveinputformat;  
-- 再执行上面的语句,占用74个map任务,消耗计算资源：slots_millis_maps=333500  

-- 2.增加map数
-- 如果a表只有一个文件大小是120m,但只有两三个字段却包含几千万条数据,一个map显然很慢
select uid,count(1),sum(case when …),sum(case when …),sum(…) from a group by uid;
-- 增加map数
set mapred.map.tasks=10;  
create table a1 as select * from a distribute by rand(123);  
-- 将a表数据随机分散到包含10个文件的a1表,占用10个map,每个map任务处理大于12m(几百万条)的数据速度快很多  

-- 3.调整reduce数
hive.exec.reducers.bytes.per.reducer;   -- 每个reduce处理的数据量,默认1g
hive.exec.reducers.max;                 -- 每个任务最大的reduce数,默认为999  
reducer数 = min(参数2,数据总量/参数1)  
-- 修改每个reduce处理的数据量,默认1g
set hive.exec.reducers.bytes.per.reducer=500000000;  
-- 设定reduce个数  
set mapred.reduce.tasks=15;  
-- 只有一个reduce的情况  
数据量小于1g、没有group by、有order by、有笛卡尔积  -- 这些都属于全局操作,hadoop不得不用一个reduce去完成
```

### 数据仓库
```bash
# 大数据平台的数据源主要来自日志数据,大数据框架解决日志数据的采集、清洗、存储、离线/实时分析
# 应用一：数据仓库 - 为公司各部门提供统一的数据出口
# 应用二：产品信息分析 - 新增用户/活跃用户/沉默用户/启动次数/版本统计/留存率/新鲜度 
# 应用三：用户行为分析 - 建立用户画像 - 精准广告推送/精准商品推荐
# 应用四：人工智能基础 - 提供大数据量的支撑,近些年硬件发展也很快

# 表的分类
实体表：对应实际业务对象,比如用户表、商品表
维度表：对应一些业务状态,也叫码表,比如地区信息、订单状态、商品分类、支付方式、审核状态
事务型事实表：随着业务发展不断产生的数据,且一旦生成就不再变化,比如订单详情、交易流水、操作日志、出入库记录
周期型事实表：随着业务发展不断产生的数据,且会随业务周期性变化,比如订单表的订单状态会更新

# 表的同步策略
实体表和维度表一般数据量都不大,可以每天存一份全量数据,即每日全量,有些万年不变的维度表可以只存一份固定值
事务型事实表由于数据量巨大且不会变化,每天同步新增数据就行,即每日增量
周期型事实表由于数据量巨大且会变化,每日全量会很冗余,每日增量又不能反应数据变化,需要使用拉链表记录每日新增量和变化量,可以获取任意历史节点的快照数据

# 函数依赖
学号  姓名  系名  系主任  课程  分数
101  张三  经济系  李强  概率论  95
102  李四  法律系  王伟  劳动法  99
完全函数依赖：(学号+课程)能推断出分数,但是单独用学号或课程推断不出分数,所以分数完全依赖于(学号,课程)
部分函数依赖：(学号+课程)能推断出姓名,但是单独用学号也能推断出姓名,所以姓名部分依赖于(学号,课程)
传递函数依赖：学号能推断出系名,系名能推断出系主任,但是系主任推断不出学号,所以系主任传递依赖于学号
# 三大范式
设计数据库表结构的规范,优点是减少数据冗余,保证数据一致性,缺点是拆分成太多小表获取数据时需要join
第一范式：列不可分割
第二范式：不能存在部分函数依赖
第三范式：不能存在传递函数依赖

# 关系建模与维度建模
关系建模主要应用于OLTP系统,为了保证数据一致性并减少冗余,mysql大部分表都遵循三范式,多表join可以依靠强大的主键索引
维度建模主要应用于OLAP系统,处理大规模数据,跨表分析统计查询会造成多表关联性能很差,通常以某个事实表为中心,维度表围绕事实表进行解释
# 星型模型、雪花模型和星座模型
维度建模又分为星型模型和雪花模型,星型模型只有一层维度,雪花模型可能有多层维度,更接近3NF但是无法完全遵守因为3NF性能较差
星座模型和前两个模型区别在于事务表数量,星座模型是基于多个事实表的,也是数据仓库的常态,和前两个模型不冲突
至于选星型模型还是雪花模型,取决于性能优先还是灵活优先,整体来看更倾向于维度更少的星型模型,hadoop体系减少join就是减少shuffle,性能差距很大
```

### 拉链表
```sql
-- 拉链表保存历史数据状态变化的同时可以节省空间
-- 1.mysql订单表结构为  
create table if not exists orders (  
order_id        int,  
create_time     string,  
update_time     string,  
status          string  
) 
stored as textfile;  

-- 2.数据仓库的ods层有一张订单分区表,存放每天的增量数据  
create table if not exists ods_orders_inc (  
order_id        int,  
create_time     string,  
update_time     string,  
status          string  
) 
partitioned by (dt string)  
stored as textfile;  

-- 3.数据仓库的dw层有一张订单历史拉链表,存放订单的所有历史数据  
create table if not exists dw_orders_his (  
order_id        int,  
create_time     string,  
update_time     string,  
status          string,
begin_date      string,  -- 该状态的生命周期开始时间
end_date        string   -- 该状态的生命周期结束时间,'9999-12-31'表示该状态仍有效,没有更新为下一个状态     
) 
stored as textfile;  

-- 查询当前所有有效记录(最新状态)
select * from dw_order_his where end_date = '9999-12-31';  
-- 查询2019-06-21历史快照(2019-06-21这一天的数据变化情况)
-- 新增: begin_date = '2019-06-21' and end_date = '9999-12-31' 
-- 更新: begin_date < '2019-06-21' and end_date = '2019-06-21'
select * from dw_order_his where begin_date <= '2019-06-21' and end_date >= '2019-06-21';

-- 全量初始化
-- 假设在20190821这天做全量初始化,那么需要将包括20190820之前的所有数据都抽取到ods并刷新到dw 
-- 1.抽取全量数据到ods  
insert overwrite table ods_orders_inc partition (dt=20190820)
select order_id,
       create_time,
       update_time,
       status
from orders
where create_time <= '2019-08-20';
-- 2.从ods刷新到dw  
insert overwrite table dw_orders_his
select order_id,
       create_time,
       update_time,
       status,
       create_time as begin_date,  
       '9999-12-31' as end_date
from ods_orders_inc
where dt=20190820;

-- 增量刷新历史数据
-- 从20190822开始,需要每天刷新前一天的更新数据到历史表
-- 1.通过增量抽取,先将2019-08-21的数据抽取到ods  
insert overwrite table ods_orders_inc partition (dt=20190821)
select order_id,
       create_time,
       update_time,
       status
from orders
where create_time = '2019-08-21' or update_time = '2019-08-21';
-- 2.关联dw_orders_his历史数据(截止到20190820)和ods_orders_inc增量数据(20190821)放入临时表  
insert overwrite table dw_orders_his_tmp
select *
from (
    -- 这是20190821这一天新增的数据,即create_time='2019-08-21',直接加进来
    select *,'2019-08-21' as begin_date,'9999-12-31' as end_date from ods_orders_inc where dt=20190821 and create_time='2019-08-21'
    union all
    -- 这是20190821这一天更新的数据以及压根就没变的那大部分历史数据
    select a.order_id,a.create_time,a.update_time,a.status,a.begin_date,  
           -- 能关联上说明状态有变化,即create_time<'2019-08-21' and update_time='2019-08-21'那部分数据,需要将end_date改成更新时间的前一天
           -- 关联不上说明状态没有变化,end_date保持不变
           case when b.order_id is not null then '2019-08-20' else a.end_date end as end_date
    from dw_orders_his a
    left join
    (select * from ods_orders_inc where dt=20190821 and create_time < '2019-08-21') b on a.order_id = b.order_id 
) t
order by order_id,begin_date; 
-- 刷新临时表数据到拉链表
insert overwrite table dw_orders_his select * from dw_orders_his_tmp;
```