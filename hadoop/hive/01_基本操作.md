- [参考文档](http://www.cnblogs.com/hpucode/p/5204871.html)
- [参考文档](http://www.cnblogs.com/xd502djj/p/3799432.html)
## basic
```bash
# 查看hive版本
hive --version
# 删除库(加cascade可以删除含有表的数据库)
drop database test cascade
# 模糊搜索表
show tables like '*name*'  
# 删除表
drop table table_name  
# 查看表详细信息
desc formatted table_name / show create table table_name    
# 添加字段(注意：添加新字段后要将原来已经存在的分区先删掉,不然数据加载不进去,如果要调整新字段顺序,可以再用change)
alter table test add columns(order_id int comment '订单id')  
# 修改字段
alter table test change column column1 column2 string comment '...' first|after column3  
# 删除字段(只保留需要的列,不需要的列删掉,同时也可以更换列的顺序)
alter table test replace columns(id int,name string)  
# 删除表分区
alter table test drop partition (dt=20160101)  # 删除单个分区  
alter table test drop partition (dt>=20160101,dt<20170101)  # 删除条件范围内的多个分区  
或者直接去hdfs上删除存储数据的目录(表的分区还在,只是没有数据)  
# 重命名表
alter table table1 rename to table2  
# 复制表结构
create table empty_table1 like table1  
# 查看分区信息
show partitions table_name  
# 查看最小分区
select min(dt) from table_name;
# 查看执行计划
explain select ...  
# 在输出结果最上面一行打印列名  
set hive.cli.print.header=true;
# 导出hive数据到本地(insert overwrite慎用,会覆盖整个目录!)
hive -e "select * from test;" > /opt/aaa.txt  
# 往mysql插入大量数据
load data local infile '/test.txt' replace into table test 	# 覆盖  
load data local infile '/test.txt' into table test          # 追加  
# 查找所有函数 
show functions 
# 查看某个函数使用案例  
desc function extended parse_url
# 视图
create view view01 as select * from debit_ifno where dt=regexp_replace(date_sub(current_date,1),'-','')  
# 注册udf,将开发的udf打成jar包上传到hdfs指定目录,然后创建函数  
create function default.url_decode as 'com.qbao.udf.decodeurl' using jar '<hdfs:///lib/decodeurl.jar>';
# load加载数据(local表示本地,否则是从hdfs上剪切;overwrite覆盖/into追加)
load data [local] inpath '...' [overwrite] into table t1 [partition]      
# insert插入数据(不能用select *,因为分区也是一列,刚开始是空表没数据,会报错column数量不一致)
insert overwrite/into table t1 select * from t2;
insert overwrite t1 partition(dt=20200101) select * from t2 where dt=20200101;
# as创建临时表
create table t2 as select ... from t1 where ...;
# sort by：分区内排序,在每个reduce内部排序
# order by：全局排序,最后会用一个reduce task来完成
hive> select * from test;  
5 3 6 2 9 8 1  
hive> select * from test order by id;  
1 2 3 5 6 8 9  
hive> set mapred.reduce.tasks=2;  
hive> select * from test sort by id;  
2 5 6 9 1 3 8  # 设定了2个reduce,从结果可以看出,在每个reduce内都做了排序,如果reduce数为1,那么两者结果是一样的  
```

## shell
| 参数                      | 说明                       |
|:-------------------------|:----------------------------|
|-d,–define <key=value>    |定义变量 -d a=b               |
|-database <databasename>   |指定数据库 默认default         |
|-e <quoted-query-string>   |执行一段sql                   |
|-f <filename>              |执行保存hql语句的文件           |
|-h,–help                  |显示帮助信息                   |
|-h <hostname>              |连接远程hive server           |
|-p <port>                  |连接远程hive server端口号      |
|-hiveconf <property=value> |设置配置参数                   |
|-hivevar <key=value>       |类似define                    |
|-s,–silent                |安静模式 不显示进度只显示结果     |

## tables
```sql
-- hive是把除select * 以外的sql都翻译成MapReduce程序在yarn集群里跑

-- 内部表
create table if not exists dw.inner(  
json string  
)  
comment '内部表'  
row format delimited  
fields terminated by '\t'  
lines terminated by '\n'  
stored as orc tblproperties ("orcompress"="snappy");  
-- orcfile比rcfile在存储结构和存储空间上做了优化
-- 内部表路径默认存放在hdfs的/user/hive/warehouse下,并且还会生成一个tmp目录

-- 外部表
create external table if not exists dw.external(
json string  
)  
comment '外部表'  
row format delimited  
fields terminated by '\t'  
lines terminated by '\n'  
stored as orc tblproperties ("orcompress"="snappy")  
location 'hdfs://nameservice1/user/flume/qbao_goods_stuff_log';
-- 外部表路径是自定义的,可以事先在hdfs上创建好

-- 分区表
create table if not exists base.rec_spu_summary(  
sumary_date         string,  
user_id             int,  
spu_id_list         array<int>  
)  
partitioned by (dt string)  
row format delimited  
fields terminated by '\001'  -- 不同列以'\001'分隔,集合(array,map)元素之间以'\002'分隔,map中key和value以'\003'分隔;  
lines terminated by '\n'  
stored as textfile;  
-- 分区表：创建表的时候可以设置分区(分区表也分内部分区表和外部分区表,如果指定location就是外部分区表)  

-- 动态分区
-- 业务需求：mysql表很大,现在要抽到hive按天分区,保留2016年后的数据,2016年以前的数据都放到20151231这个分区里
-- 解决方法：先将全量数据导入到temp的临时表(不分区),然后使用动态分区插入到ods层的分区表中
-- 注意：动态分区的字段一定位于其他各个字段的最后
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;       -- 设置允许动态分区 
set hive.optimize.sort.dynamic.partition=true;        -- 设置动态分区排序优化 
set hive.exec.max.dynamic.partitions=10000;            -- 总共的最大动态分区数  
set hive.exec.max.dynamic.partitions.pernode=10000;    -- 每个节点能生成的最大分区数  
insert overwrite table ods.tickets_order partition(dt)
select *,  
       case when create_time >= '2016-01-01' then regexp_replace(substr(create_time,0,10),'-','') else 20151231 end
from temp.tickets_order;

-- mysql表
create table if not exists app_v40_index_localtion_pvuv_sum (
	stat_date date not null comment '日期',  
	mobile_type varchar (20) not null default '' comment '手机类型',  
	pv int (11) default null,  
	primary key (stat_date)  
) engine = innodb default charset = utf8 comment='呵呵'  
```

## join
```sql
       id     id
1   a   3      1   xxx   2
2   b   4      2   yyy   3 
3   c   1      3   zzz   5

-- 内连接(交集)
hive> select * from a join b on(a.id = b.id);  
3   c   1   1   xxx   2  
1   a   3   3   zzz   5  

-- 左连接(左表是主表)
hive> select * from a left join b on(a.id = b.id);  
1   a   3   3   zzz   5  
2   b   4   null null null  
3   c   1   1   xxx    2  

-- 左半开连接：只显示匹配到的左表数据,比左连接快
hive> select * from a left semi join b on(a.id = b.id);  
1   a   3  
3   c   1  

-- 右连接(右表是主表)
hive> select * from a right join b on(a.id = b.id);  
3   c   1       1   xxx   2  
null null null  2   yyy   3  
1   a   3       3   zzz   5  

-- 全连接(并集)
hive> select * from a full join b on(a.id = b.id);  
3   c   1      1   xxx   2  
null null null 2   yyy   3  
1   a   3      3   zzz   5  
2   b   4      null null null  

-- 笛卡尔积：m*n(没有关联条件)
hive> select * from a join b;  
1   a   3   1   xxx   2  
2   b   4   1   xxx   2  
3   c   1   1   xxx   2  
1   a   3   2   yyy   3  
2   b   4   2   yyy   3  
3   c   1   2   yyy   3  
1   a   3   3   zzz   5  
2   b   4   3   zzz   5  
3   c   1   3   zzz   5  
```

## mapred task
```sql
-- map join操作小表放左边,会被加载到内存,在map端做join操作而不是reduce端,可以省去shuffle过程大量io操作  
-- map和reduce数量不是越多越好,启动和初始化很消耗时间和资源;并且有多少个reduce就会有多少个output文件,迭代过程中大量output文件又会成为下个任务的input
-- job会通过input文件产生map任务,map数和文件大小,文件个数,文件块大小(默认128m,set dfs.block.size)有关  
-- 原则：使大数据量利用合适的map/reduce数,使单个map/reduce任务处理合适的数据量

-- 1.减少map数  
-- inputdir /user/hive/warehouse/test/dt=20170101 共194个文件总大小9g,其中很多<<128m的,正常执行会占用194个map任务,消耗计算资源：slots_millis_maps=623020  
select count(1) from test where dt=20170101;  
-- 设置块大小
set mapred.max.split.size=100000000;  
set mapred.min.split.size.per.node=100000000;  
set mapred.min.split.size.per.rack=100000000;  
-- 合并小文件  
set hive.input.format=org.apache.hadoop.hive.ql.io.combinehiveinputformat;  
-- 再执行上面的语句,占用74个map任务,消耗计算资源：slots_millis_maps=333500  

-- 2.增加map数
-- 如果a表只有一个文件大小是120m,但只有两三个字段却包含几千万条数据,一个map显然很慢
select user_id,count(1),sum(case when …),sum(case when …),sum(…) from a group by user_id;
-- 增加map数
set mapred.map.tasks=10;  
create table a1 as select * from a distribute by rand(123);  
-- 将a表数据随机分散到包含10个文件的a1表,占用10个map,每个map任务处理大于12m(几百万条)的数据速度快很多  

-- 3.调整reduce数
hive.exec.reducers.bytes.per.reducer;   -- 每个reduce处理的数据量,默认1G
hive.exec.reducers.max;                 -- 每个任务最大的reduce数,默认为999  
reducer数 = min(参数2,数据总量/参数1)  
-- 修改每个reduce处理的数据量,默认1g
set hive.exec.reducers.bytes.per.reducer=500000000;  
-- 设定reduce个数  
set mapred.reduce.tasks=15;  
-- 只有一个reduce的情况  
数据量小于1G、没有group by和order by操作、有笛卡尔积
```

## data skew
- 空值导致数据倾斜
```sql  
-- 方案一  
select * from users a join orders b on a.user_id is not null and a.user_id = b.user_id  
union all  
select * from users a where a.user_id is null;  
-- 方案二  
select * from users a left join orders b on case when a.user_id is null then concat('hive',rand()) else a.user_id = b.user_id;  
-- 方案一是2个job,方案二是1个job且io更少,所以方案二更好  
```
- 不同数据类型关联导致数据倾斜
```sql  
-- 比如users表是int类型,而orders表是string类型 
select * from users a left join orders b on a.user_id = cast(b.user_id as int) ;  
```
- 参数优化
```sql  
-- 1.并发执行  
set hive.exec.parallel=true;  
set hive.exec.parallel.thread.number=8;
-- 2.聚合操作  
set hive.map.aggr=true; -- 在map中会做部分聚合操作,效率更高但更消耗内存  
-- 3.数据倾斜(大量key被shuffle到某个reduce)  
set mapred.reduce.tasks= 200;                    -- 增大reduce个数  
set hive.optimize.skewjoin=true;                 -- join过程出现倾斜设置为true  
set hive.groupby.skewindata=true;                -- group by过程出现倾斜设置为true  
join、group by、count distinct 操作都可能会引起数据倾斜问题  
-- 生成的查询计划会有两个mrjob,第一个mrjob中map的输出结果会随机分配到reduce,每个reduce做聚合操并输出结果,这样即使相同的groupby key也会被分配到不同的reduce,从而达到负载均衡
-- 第二个mrjob再根据预处理的数据结果按照groupby key分配到reduce,这个过程可以保证相同的groupby key被分配到同一个reduce,完成最终聚合操作  
set hive.skewjoin.key=100000;                    -- join的键对应的记录数超过设定值(具体看集群配置)会做分拆  
set hive.groupby.mapaggr.checkinterval=100000;   -- groupby的键对应的记录数超过设定值会做分拆  
```

## hdfs
```bash
# Client：客户端
1.将文件按block块切分
2.与NameNode交互,获取文件的位置信息
3.与DataNode交互,读写数据
# NameNode：管理节点
1.管理hdfs命名空间
2.管理元数据信息,即文件与数据块的映射关系
3.配置数据块副本 
4.处理客户端读写请求
# DataNode：工作节点,执行NameNode下达的命令
1.存储实际的数据块
2.执行数据块的读/写操作
# SecondaryNameNode：为了避免edits不断变大,会定期合并fsimage和edits,该操作挺耗时,在NameNode操作会影响性能导致卡顿
fsimage是某一时刻hdfs的快照
edits会记录hdfs的各种更新操作
        
# 写数据
# block大小 dfs.block.size=128m
# packet大小 dfs.write.packet.size=128k
1.客户端请求NameNode上传文件,NameNode返回是否可以上传
2.客户端请求上传第一个block的DataNode服务器,NameNode返回dn1/dn2/dn3数据节点
3.客户端请求向dn1上传数据,dn1收到请求会继续调用dn2,然后dn2调用dn3,建立通信管道pipline
4.客户端开始往dn1上传第一个block,以packet为单位,dn1收到一个packet就会传给dn2,dn2传给dn3 
5.当一个block传输完成之后,客户端再次请求NameNode上传第二个block的服务器

# 读数据
1.客户端请求NameNode下载文件
2.NameNode查询元数据找到文件块存放地址,就近挑选一台dn节点读取数据
3.DataNode开始传输数据给客户端
```