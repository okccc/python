- [cdh各组件端口](https://docs.cloudera.com/documentation/enterprise/6/6.2/topics/cdh_ports.html)
### hdfs
```bash
# Client：客户端
1.将文件按block块切分
2.与NameNode交互,获取文件的位置信息
3.与DataNode交互,读写数据
# NameNode：管理节点
1.管理hdfs命名空间
2.管理元数据信息,即文件与数据块的映射关系
3.配置数据块副本 
4.处理客户端读写请求
# DataNode：工作节点,执行NameNode下达的命令
1.存储实际的数据块
2.执行数据块的读写操作
# SecondaryNameNode：为了避免edits不断变大,会定期合并fsimage和edits,该操作挺耗时,在NameNode操作会影响性能导致卡顿
fsimage是某一时刻hdfs的快照
edits会记录hdfs的各种更新操作
# balancer
使hdfs各节点的空间使用率均匀分布
        
# 写数据
# block大小 dfs.block.size=128m
# packet大小 dfs.write.packet.size=128k
1.客户端请求NameNode上传文件,NameNode返回是否可以上传
2.客户端请求上传第一个block的DataNode服务器,NameNode返回dn1/dn2/dn3数据节点
3.客户端请求向dn1上传数据,dn1收到请求会继续调用dn2,然后dn2调用dn3,建立通信管道pipline
4.客户端开始往dn1上传第一个block,以packet为单位,dn1收到一个packet就会传给dn2,dn2传给dn3 
5.当一个block传输完成之后,客户端再次请求NameNode上传第二个block的服务器

# 读数据
1.客户端请求NameNode下载文件
2.NameNode查询元数据找到文件块存放地址,就近挑选一台dn节点读取数据
3.DataNode开始传输数据给客户端
```

### hive
```bash
# Hive、HiveServer2、Beeline
hiveserver是本地模式,必须到其所在机器连接访问,hivecli连接的就是hiveserver
hiveserver2是远程连接模式,可以在任意机器使用beeline/java/python通过jdbc连接
# Metastore
存储hive元数据,一般存放在关系型数据库mysql中,内置derby只支持一个会话连接
# Gateway
由于hive服务没有worker角色,需要另一种机制使客户端的配置传播到集群中其它主机
# Driver
包括Compiler,Optimizer,Executor,编译和优化hql语句并生成执行计划,调用MR框架计算

# 行列数据库对比
数据库以行列二维表形式展现数据,但是以一维字符串方式存储数据
行式存储：把一行的数据值串在一起存储,然后再存储下一行(1,zhangsan,18;2,lisi,19;3,wangwu,20...)
        读数据会读取一整行有些冗余,写数据会一次性写入速度很快,适合写多读少,面向OLTP(mysql)
列式存储：把一列的数据值串在一起存储,然后再存储下一列(1,2,3 zhangsan,lisi,wangwu 18,19,20 ...)
        读数据只会读取相关列避免全表扫描且无需维护索引,写数据要把一行数据拆成单列多次写入消耗更大,适合读多写少,面向OLAP(hive/hbase)
        由于同一列数据重复度很高且空值可以不存储,可以压缩数据节省空间
# 文件存储格式
TEXTFILE和SEQUENCEFILE基于行存储(用得少)
ORC和PARQUET基于列存储(用的多)
```
```sql
-- beeline的显示格式更友好
beeline> !connect jdbc:hive2://master1:10000
Connecting to jdbc:hive2://master1:10000
Enter username for jdbc:hive2://master1:10000: 
Enter password for jdbc:hive2://master1:10000: 
Connected to: Apache Hive (version 1.1.0-cdh5.14.2)
Driver: Hive JDBC (version 1.1.0-cdh5.14.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://master1:10000> SHOW DATABASES;
+----------------+--+
| database_name  |
+----------------+--+
| default        |
| dm             |
| dw             |
| ods            |
| tmp            |
| views          |
+----------------+--+
6 rows selected (0.293 seconds)
0: jdbc:hive2://master1:10000> !quit
Closing: 0: jdbc:hive2://master1:10000
```

### impala
```bash
# d(daemon)表示守护进程,是运行在linux后台的一种服务程序,周期性地执行某种任务或等待处理某些事件,linux的大多数服务就是用守护进程实现的
# impala是基于hive的实时分析查询引擎,直接使用hive的元数据库metadata
# hive和impala对比
hive适合长时间的批处理,有大量读写磁盘的过程 map->shuffle->reduce->map->shuffle->reduce
impala适合实时查询,中间结果放内存通过网络传输没有磁盘读写所以速度更快

# Impala Daemon(核心组件,多实例,物理进程impalad)
负责读写数据,接收从impala-shell/hue/jdbc等接口发送的查询sql,并行查询并在集群中分配任务,将中间结果集发送给协调器
impala daemon通常和datanode部署在相同节点,接收查询的节点作为协调器coordinator,其它节点会传输中间结果集给协调器并由协调器构建最终结果集
impala daemon会不间断地和statestore通信以确认哪些节点是健康的能接收新的任务,同时接收catalog传来的广播消息更新元数据

# Impala Statestore(单实例,物理进程statestored)
检查集群各节点Impala daemon的健康状态

# Impala Catalog(单实例,物理进程catalogd)
当metadata更新时会通知任意impala daemon刷新元数据信息,因为会与statestore交互所以安装在同一节点
```

### shell
```bash
# hive shell
-d,--define<key=value>         # 定义变量 -d num=10 
-e,<quoted-query-string>       # 执行一段sql
-f,--filename                  # 执行保存sql的文件           
-h,--hostname                  # 连接远程hive server           
-p,--port                      # 连接远程hive server端口号      
-hiveconf,<property=value>     # 设置配置参数                   
-hivevar,<key=value>           # 类似define                    
-S,--silent                    # 安静模式,只显示结果不显示进度

# impala-shell
-B,--delimited                 # 去格式化输出,大数据量查询可以提高效率
-i,--impalad                   # 连接指定impalad,impala-shell端口21000,hue/jdbc端口21050
-d,--database                  # 指定数据库
-q,--query                     # 执行一段sql
-f,--filename                  # 执行保存sql的文件
-o,--output                    # 输出结果到指定文件
-v/-V,--version/--verbose      # 查看版本信息,开启详细输出
--quiet                        # 安静模式,只显示结果不显示进度
```

### cmd
```sql
-- 删除库(加cascade可以删除含有表的数据库)
hive> drop database test cascade
-- 模糊搜索表
hive> show tables like '*name*'  
-- 删除表
hive> drop table test
-- 清空表数据
hive> truncate table test
-- 添加字段(注意：添加新字段后要将原来已经存在的分区先删掉,不然数据加载不进去,如果要调整新字段顺序,可以再用change)
hive> alter table test add columns(order_id int comment '订单id')  
-- 修改字段
hive> alter table test change column column1 column2 string comment '...' first|after column3  
-- 删除字段(只保留需要的列,不需要的列删掉,同时也可以更换列的顺序)
hive> alter table test replace columns(id int, name string)  
-- 删除表分区,如果直接删除hdfs数据目录,表分区还在但没有数据 
hive> alter table test drop partition (dt=20160101)               # 删除单个分区
hive> alter table test drop partition (dt>=20160101,dt<20170101)  # 删除多个分区
-- 重命名表
hive> alter table table1 rename to table2  
-- temporary表示临时表,仅在本次hive session期间有效,关闭hive后会自动删除,不加该关键字则会存储下来
-- create table like 复制表结构(没有数据)
hive> create temporary table table1_tmp like table1 stored as textfile
-- create table as 生成新表并插入数据,表结构取决于select的内容
hive> create temporary table table1_tmp as select * from table1;
-- 查看分区信息
hive> show partitions test  
-- 查看最小分区
hive> select min(dt) from test
-- 查看某项配置信息
hive> set mapred.reduce.tasks
-- 在输出结果最上面一行打印列名  
hive> set hive.cli.print.header=true
-- 查找所有函数 
hive> show functions 
-- 查看某个函数使用案例  
hive> desc function extended parse_url
-- 视图
hive> create view v01 as select * from debit_info where dt=regexp_replace(date_sub(current_date,1),'-','')  
-- 注册udf,将开发的udf打成jar包上传到hdfs指定目录,然后创建函数  
hive> create function default.url_decode as 'com.qbao.udf.decodeurl' using jar '<hdfs:///lib/decodeurl.jar>';

-- hive查看表统计信息
hive> desc formatted table_name;
-- hive计算表和字段的统计信息,分区表必须指定分区(不可靠且难用,建议用impala的compute stats)
hive> analyze table table_name [partition(dt=20200612)] compute statistics;  -- 表
hive> analyze table table_name [partition(dt=20200612)] compute statistics for columns;  -- 列

-- impala连接指定主机
[master2:21000] > connect namenode1:21000;
-- 刷新指定表
[master2:21000] > refresh table_name;
-- 刷新元数据所有表
[master2:21000] > invalidate metadata;
-- 查看执行计划
[master2:21000] > explain select;
-- 设置查询计划显示级别
[master2:21000] > set explain_level=0/1/2/3;  -- 等级越高越详细
-- 执行查询sql后再执行summary或profile可以查看详细查询分析
[master2:21000] > summary/profile;
-- impala优化器：hive数据更新 - refresh - compute stats 会统计一些聚合信息并存储在元数据中
-- impala关联查询优化：compute stats收集统计信息后,impala会基于每个表的大小、每一列的不同值个数等信息优化查询计划
-- 建议最大表放首位,因为这个表是直接从磁盘读取,它的大小不影响内存使用,后续join的表作为中间结果都是放在内存
[master2:21000] > compute/drop stats table_name;  -- 全量
[master2:21000] > compute/drop incremental stats table_name partition(dt='20200612' | dt>'20200101' | dt<'20200612');  -- 增量
-- impala查看表和字段统计信息
[master2:21000] > show column stats table_name;   -- 查看字段聚合信息
[master2:21000] > show table stats table_name;  -- 查看表聚合信息,包括文件行数/大小/类型/路径等

-- mysql数据导入
mysql> show variables like 'secure_file_priv';
mysql> load data local infile '...' [replace] into table test  # 覆盖/追加 
-- mysql数据导出
mysql> select * from test into outfile '...' FIELDS TERMINATED BY ',' ENCLOSED BY '"' LINES TERMINATED BY '\n';  
-- hive数据导入
hive> load data [local] inpath '...' [overwrite] into table t1 [partition(dt='..')]  # 本地(复制)/hdfs(剪切)  覆盖/追加    
hive> insert overwrite/into table t1 [partition(dt=20200101)] select * from t2 where ...
hive> create table t2 as select * from t1 where ...
-- hive数据导出
hive -e "select * from test;" > /opt/aaa.txt  # insert overwrite慎用,会覆盖整个目录!

-- 排序方式
hive> select * from test;  
5 3 6 2 9 8 1  
-- order by：全局排序,最后会用一个reduce task来完成
hive> select * from test order by id;  
1 2 3 5 6 8 9  
-- sort by：分区内排序,在每个reduce内部排序,如果reduce.task=1,等价于order by
-- distribute by：对map输出数据按指定字段划分到不同reduce中,常和sort by一起使用
hive> set mapred.reduce.tasks=2;  
hive> select * from test distribute by id sort by id;  
2 5 6 9 1 3 8    
-- cluster by：当distribute by 和sorts by字段相同时可使用cluster by代替,但是只能升序排序
```

### tables
```sql
-- hive是把除了类似select * 这种简单查询以外的sql都翻译成MapReduce在yarn集群里跑

-- 内部表,默认路径hive.metastore.warehouse.dir=/user/hive/warehouse,并且还会生成一个tmp目录
-- 内部表数据由hive管理,删除表会直接删除hive元数据metadata及存储数据
create table if not exists dw.inner(  
json string  
)  
comment ''  
row format delimited  
fields terminated by '\t'  
lines terminated by '\n'  
-- ORC：先将数据按行分块再按列存储,保证同一条记录在一个块上避免读取多个数据块,SNAPPY可以极大节省存储空间
stored as orc tblproperties ("orc.compress"="SNAPPY");
-- 注意：sqoop从mysql导到hive的表不能是orc格式,不然无法访问数据,可以先导到textfile临时表再insert overwrite/into到orc表

-- 外部表,可以指定路径
-- 外部表数据由hdfs管理,删除表只会删除hive元数据metadata,hdfs上的文件还在
create external table if not exists dw.external(
json string  
)  
comment ''  
row format delimited  
fields terminated by '\t'  
lines terminated by '\n'  
stored as orc tblproperties ("orc.compress"="SNAPPY")  
location 'hdfs://nameservice1/user/flume/qbao_goods_stuff_log';

-- 分区表可以在海量数据当中快速检索到需要的数据
-- 内部表和外部表都可以设置分区,指定location就是外部分区表  
-- 分区：以字段的形式在表结构中存在,但该字段不存放实际数据内容,仅仅是分区标识
-- 分桶：更为细粒度的数据范围划分,将整个数据按照某个列的hash值划分到指定数量的桶里(不常用)
create table if not exists ods.rec_spu_summary(  
sumary_date         string,  
uid                 int,  
spu_id_list         array<int>  
)  
partitioned by (dt string)  
row format delimited  
fields terminated by '\001'  -- 不同列以'\001'分隔,集合(array,map)元素之间以'\002'分隔,map中key和value以'\003'分隔;  
lines terminated by '\n'  
stored as textfile;  

-- 动态分区
-- 业务需求：mysql表很大,现在要抽到hive按天分区,保留2016年后的数据,2016年以前的数据都放到20151231这个分区里
-- 解决方法：先将全量数据导入到temp的临时表(不分区),然后使用动态分区插入到ods层的分区表中
-- 注意：动态分区的字段一定位于其他各个字段的最后
set hive.exec.dynamic.partition=true;                  -- 开启动态分区
set hive.exec.dynamic.partition.mode=nonstrict;        -- 默认strict(严格模式,必须至少包含一个静态分区) 
set hive.exec.max.dynamic.partitions=10000;            -- 能生成的动态分区最大总数  
set hive.exec.max.dynamic.partitions.pernode=1000;     -- 每个节点能生成的最大分区数 
insert overwrite table ods.tickets_order partition(dt)
select *,  
       case when create_time >= '2016-01-01' then regexp_replace(substr(create_time,0,10),'-','') else 20151231 end
from temp.tickets_order;

-- mysql表
create table if not exists app_v40_index_localtion_pvuv_sum (
	stat_date date not null comment '日期',  
	mobile_type varchar (20) not null default '' comment '手机类型',  
	pv int (11) default null,  
	primary key (stat_date)  
) engine = innodb default charset = utf8 comment='呵呵'  
```

### join
```sql
       id     id
1   a   3      1   xxx   2
2   b   4      2   yyy   3 
3   c   1      3   zzz   5

-- 内连接(id交集)
hive> select * from a join b on(a.id = b.id);  
3   c   1   1   xxx   2  
1   a   3   3   zzz   5  

-- 左连接(左表是主表)
hive> select * from a left join b on(a.id = b.id);  
1   a   3   3   zzz   5  
2   b   4   null null null  
3   c   1   1   xxx    2  

-- 左半开连接：只显示匹配到的左表数据,比左连接快
hive> select * from a left semi join b on(a.id = b.id);  
1   a   3  
3   c   1  

-- 右连接(右表是主表)
hive> select * from a right join b on(a.id = b.id);  
3   c   1       1   xxx   2  
null null null  2   yyy   3  
1   a   3       3   zzz   5  

-- 全连接(id并集)
hive> select * from a full join b on(a.id = b.id);  
3   c   1      1   xxx   2  
null null null 2   yyy   3  
1   a   3      3   zzz   5  
2   b   4      null null null  

-- 笛卡尔积：m*n(没有关联条件)
hive> select * from a join b;  
1   a   3   1   xxx   2  
2   b   4   1   xxx   2  
3   c   1   1   xxx   2  
1   a   3   2   yyy   3  
2   b   4   2   yyy   3  
3   c   1   2   yyy   3  
1   a   3   3   zzz   5  
2   b   4   3   zzz   5  
3   c   1   3   zzz   5  
```

### skew
```sql
-- 数据倾斜本质：shuffle过程中map端的输出结果按照key hash分配不均匀,导致reduce端分到的数据量差异过大
-- 1.join导致
set hive.optimize.skewjoin=true; 
set hive.skewjoin.key=100000;  -- join的键对应的记录数超过设定值会做map join 
-- a)关联字段存在null值
select * from users a left join orders b on a.uid is not null and a.uid = b.uid  
union all  
select * from users a where a.uid is null;  
-- 上面方案需要2个job,下面方案只要1个job  
select * from users a left join orders b on case when a.uid is null then concat('hive',rand()) else a.uid = b.uid;  
-- b)关联字段类型不一致,类型转换时会产生null值
select * from users a left join orders b on a.uid = cast(b.uid as int) ;  
-- c)小表前置
hive join默认是reduce join,如果左表很小可以将其加载到内存做map join,省掉shuffle过程

-- 2.group by导致
set hive.map.aggr=true;  -- 在map端先做部分聚合操作
set hive.groupby.mapaggr.checkinterval=100000;   -- groupby的键对应的记录数超过设定值就会在map端聚合 
set hive.groupby.skewindata=true;  -- 生成的查询计划会有两个mrjob,第一个job中map的输出结果会随机分配到reduce,每个reduce做聚合操作并输出结果,这样即使相同的groupby key也会被分配到不同的reduce,从而达到负载均衡,第二个job再根据预处理的数据结果按照groupby key分配到reduce,这个过程可以保证相同的groupby key被分配到同一个reduce,完成最终聚合操作  
-- multi-group可以将多个group by放到一个mapreduce
from area
insert overwrite table t1 
select provice,count(rainfall) from area where dt=20200101 group by provice
insert overwrite table t2
select provice,city,count(rainfall) from area where dt=20200101 group by provice,city

-- 3.count(distinct ..)导致
count distinct属于全聚合操作,最终会用一个reduce处理导致耗时过长,可以先用group by去重
select count(id) from (select id from users group by id) a;  -- 虽然会多一个job但是数据量很大时还是值得的
```

### optimize(粗粒度)
```sql
-- hive优化本质上是mapreduce优化
-- map join操作小表放左边,会被加载到内存,在map端做join操作而不是reduce端,可以省去shuffle过程大量io操作  
-- map和reduce数量不是越多越好,启动和初始化很消耗时间和资源;并且有多少个reduce就会有多少个output文件,迭代过程中大量output文件又会成为下个任务的input
-- job会通过input文件产生map任务,map数和文件大小,文件个数,文件块大小(默认128m,set dfs.block.size)有关  
-- 原则：使大数据量利用合适的map/reduce数,使单个map/reduce任务处理合适的数据量

-- 1.减少map数  
-- inputdir /user/hive/warehouse/test/dt=20170101 共194个文件总大小9g,其中很多<<128m的,正常执行会占用194个map任务,消耗计算资源：slots_millis_maps=623020  
select count(1) from test where dt=20170101;  
-- 设置块大小
set mapred.max.split.size=100000000;  
set mapred.min.split.size.per.node=100000000;  
set mapred.min.split.size.per.rack=100000000;  
-- 合并小文件  
set hive.input.format=org.apache.hadoop.hive.ql.io.combinehiveinputformat;  
-- 再执行上面的语句,占用74个map任务,消耗计算资源：slots_millis_maps=333500  

-- 2.增加map数
-- 如果a表只有一个文件大小是120m,但只有两三个字段却包含几千万条数据,一个map显然很慢
select uid,count(1),sum(case when …),sum(case when …),sum(…) from a group by uid;
-- 增加map数
set mapred.map.tasks=10;  
create table a1 as select * from a distribute by rand(123);  
-- 将a表数据随机分散到包含10个文件的a1表,占用10个map,每个map任务处理大于12m(几百万条)的数据速度快很多  

-- 3.调整reduce数
hive.exec.reducers.bytes.per.reducer;   -- 每个reduce处理的数据量,默认1G
hive.exec.reducers.max;                 -- 每个任务最大的reduce数,默认为999  
reducer数 = min(参数2,数据总量/参数1)  
-- 修改每个reduce处理的数据量,默认1g
set hive.exec.reducers.bytes.per.reducer=500000000;  
-- 设定reduce个数  
set mapred.reduce.tasks=15;  
-- 只有一个reduce的情况  
数据量小于1G、没有group by、有order by、有笛卡尔积  -- 这些都属于全局操作,hadoop不得不用一个reduce去完成
```